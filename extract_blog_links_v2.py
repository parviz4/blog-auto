#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import concurrent.futures

BLOG_URL = "https://rasta4u.blogfa.com"
MAX_WORKERS = 10

def is_internal_post_link(url):
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ù„ÛŒÙ†Ú© ÛŒÚ© Ù¾Ø³Øª Ø¯Ø§Ø®Ù„ÛŒ ÙˆØ¨Ù„Ø§Ú¯ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±."""
    parsed_url = urlparse(url)
    return parsed_url.path.startswith('/post/')

def check_link_health(url):
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ù„ÛŒÙ†Ú© Ø³Ø§Ù„Ù… Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±."""
    try:
        response = requests.head(url, timeout=8, allow_redirects=True)
        if response.status_code == 200:
            print(f"  âœ… [200] {url.split('/')[-1]}")
            return url
    except requests.RequestException:
        pass
    return None

def extract_blog_links():
    """Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙˆØ¨Ù„Ø§Ú¯ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø³Ù„Ø§Ù…Øª Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    print("ğŸ” Ø¯Ø±Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙˆØ¨Ù„Ø§Ú¯...\n")
    try:
        response = requests.get(BLOG_URL, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        post_links = []
        seen_urls = set()

        for link in soup.find_all('a', href=True):
            url = link['href']
            text = link.get_text(strip=True)

            full_url = urljoin(BLOG_URL, url)

            if is_internal_post_link(full_url) and text and full_url not in seen_urls:
                post_links.append({'title': text.strip(), 'url': full_url})
                seen_urls.add(full_url)

        print(f"ğŸ”— {len(post_links)} Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª...\n")

        active_links = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(check_link_health, item['url']): item for item in post_links}
            for future in concurrent.futures.as_completed(future_to_url):
                item = future_to_url[future]
                if future.result():
                    active_links.append(item)

        with open('active_blog_links.json', 'w', encoding='utf-8') as f:
            json.dump({'posts': active_links, 'total': len(active_links)}, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… {len(active_links)} Ù„ÛŒÙ†Ú© Ø³Ø§Ù„Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n")
        return active_links

    except requests.RequestException as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ ÙˆØ¨Ù„Ø§Ú¯: {e}\n")
        return []

if __name__ == '__main__':
    extract_blog_links()
